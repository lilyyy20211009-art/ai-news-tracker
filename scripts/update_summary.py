"""
AI News Aggregator - æ‘˜è¦ç”Ÿæˆå™¨
è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„ AI çƒ­ç‚¹æ‘˜è¦å¹¶æ›´æ–°åˆ° index.html å’Œ today.html
"""

import html
import json
import os
import re
import unicodedata
import yaml
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# DeepSeek API é…ç½®
def load_api_key():
    """ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åŠ è½½ API Key"""
    # å…ˆå°è¯•ç¯å¢ƒå˜é‡
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if api_key:
        return api_key, os.environ.get("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")

    # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
    config_paths = [
        Path(__file__).parent.parent / "config.yaml",
        Path("~/config.yaml").expanduser(),
    ]

    for config_path in config_paths:
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    api_key = config.get("llm", {}).get("api_key", "")
                    base_url = config.get("llm", {}).get("base_url", "https://api.deepseek.com")
                    if api_key:
                        return api_key, base_url
            except Exception as e:
                print(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    return None, None

DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL = load_api_key()
if DEEPSEEK_BASE_URL and not DEEPSEEK_BASE_URL.endswith("/v1"):
    DEEPSEEK_BASE_URL = DEEPSEEK_BASE_URL + "/v1"


# è‹±æ–‡æ ‡é¢˜åˆ°ä¸­æ–‡çš„ç®€å•ç¿»è¯‘æ˜ å°„
# æ³¨æ„ï¼šè¿™é‡Œçš„ key å¿…é¡»ä¸ HTML å®ä½“è§£ç åçš„æ ‡é¢˜å®Œå…¨åŒ¹é…
TITLE_TRANSLATIONS = {
    # The Verge - ç²¾ç¡®åŒ¹é…è§£ç åçš„æ ‡é¢˜
    "Google's annual revenue tops $400 billion for the first time": "Google å¹´æ”¶å…¥é¦–æ¬¡çªç ´ 4000 äº¿ç¾å…ƒ",
    "Sam Altman responds to Anthropic's 'funny' Super Bowl ads": "Sam Altman å›åº” Anthropic è¶…çº§ç¢—å¹¿å‘Š",
    "OpenClaw's AI 'skill' extensions are a security nightmare": "OpenClaw AI æ‰©å±•å­˜åœ¨ä¸¥é‡å®‰å…¨é—®é¢˜",
    "GitHub adds Claude and Codex AI coding agents": "GitHub æ·»åŠ  Claude å’Œ Codex AI ç¼–ç¨‹åŠ©æ‰‹",
    "Anthropic says 'Claude will remain ad-free,' unlike ChatGPT": "Anthropic æ‰¿è¯º Claude å°†æ°¸è¿œæ— å¹¿å‘Š",
    "Sen. Warren wants to know what Google Gemini's built-in checkout means for user privacy": "å‚è®®å‘˜ Warren è´¨ç–‘ Google Gemini ç»“è´¦åŠŸèƒ½éšç§é—®é¢˜",

    # TechCrunch
    "Sam Altman got exceptionally testy over Claude Super Bowl ads": "Sam Altman å¯¹ Claude è¶…çº§ç¢—å¹¿å‘Šååº”å¼ºçƒˆ",
    "Alphabet won't talk about the Google-Apple AI deal, even to investors": "Alphabet æ‹’ç»è°ˆè®º Google-Apple AI åˆä½œ",
    "Google's Gemini app has surpassed 750M monthly active users": "Google Gemini æœˆæ´»ç”¨æˆ·è¶… 7.5 äº¿",
    "Meet Gizmo: A TikTok for interactive, vibe-coded mini apps": "Gizmoï¼šç±»ä¼¼ TikTok çš„äº¤äº’å¼åº”ç”¨å¹³å°",
    "AI SRE Resolve AI confirms $125M raise, unicorn valuation": "Resolve AI è· 1.25 äº¿ç¾å…ƒèèµ„ï¼Œä¼°å€¼è¾¾ç‹¬è§’å…½",
    "Amazon to begin testing AI tools for film and TV production next month": "Amazon å°†å¼€å§‹æµ‹è¯•å½±è§†åˆ¶ä½œ AI å·¥å…·",
    "A16z just raised $1.7B for AI infrastructure": "A16z ç­¹é›† 17 äº¿ç¾å…ƒä¸“æ³¨ AI åŸºç¡€è®¾æ–½",
    "ElevenLabs raises $500M from Sequoia at an $11 billion valuation": "ElevenLabs èèµ„ 5 äº¿ç¾å…ƒï¼Œä¼°å€¼è¾¾ 110 äº¿ç¾å…ƒ",
    "Alexa+, Amazon's AI assistant, is now available to everyone in the US": "Alexa+ AI åŠ©æ‰‹å‘å…¨ç¾å¼€æ”¾",
    "Tinder looks to AI to help fight 'swipe fatigue' and dating app burnout": "Tinder ä½¿ç”¨ AI å¯¹æŠ—æ»‘åŠ¨ç–²åŠ³",
    "ChatGPT now lets you call the AI for free": "ChatGPT ç°åœ¨æ”¯æŒå…è´¹è¯­éŸ³é€šè¯",
    "OpenAI in 'advanced talks' to host a data center with Oracle": "OpenAI ä¸ Oracle æ´½è°ˆå»ºè®¾æ•°æ®ä¸­å¿ƒ",
    "Former Character.AI founders launch a new educational AI startup": "Character.AI è”åˆåˆ›å§‹äººæ¨å‡ºæ•™è‚² AI åˆ›ä¸šå…¬å¸",

    # NYT
    "Google Plans to Double Spending Amid A.I. Race": "Google è®¡åˆ’åœ¨ AI ç«èµ›ä¸­åŠ å€æŠ•å…¥",
    "Babies, Robots and Climate Change": "å©´å„¿ã€æœºå™¨äººä¸æ°”å€™å˜åŒ–",
    "Why A.I. Fears Are Battering Stocks, Again": "AI ææƒ§å†æ¬¡å†²å‡»è‚¡å¸‚",
    "Bedrock, an A.I. Start-Up for Construction, Raises $270 Million": "Bedrock æœºå™¨äººå…¬å¸èèµ„ 2.7 äº¿ç¾å…ƒ",
    "A.I. Loves Fake Images. But They've Been a Thing Since Photography Began.": "AI ä¸è™šå‡å›¾ç‰‡çš„å†å²",
}


def normalize_quotes(text: str) -> str:
    """å°†å„ç§å¼•å·è§„èŒƒåŒ–ä¸ºæ ‡å‡†çš„ç›´å¼•å·"""
    # Curly quotes to straight quotes mapping
    quote_map = {
        '\u2018': "'",  # Left single quotation mark
        '\u2019': "'",  # Right single quotation mark
        '\u201c': '"',  # Left double quotation mark
        '\u201d': '"',  # Right double quotation mark
        '\u0060': "'",  # Grave accent
        '\u00b4': "'",  # Acute accent
        '\u201a': ',',  # Single low-9 quotation mark
        '\u201b': "'",  # Single high-reversed-9 quotation mark
        '\u201e': '"',  # Double low-9 quotation mark
        '\u201f': '"',  # Double high-reversed-9 quotation mark
    }
    for curly, straight in quote_map.items():
        text = text.replace(curly, straight)
    return text


def translate_with_deepseek(title: str) -> str:
    """
    ä½¿ç”¨ DeepSeek API ç¿»è¯‘è‹±æ–‡æ ‡é¢˜ä¸ºä¸­æ–‡

    Args:
        title: è‹±æ–‡æ ‡é¢˜

    Returns:
        ä¸­æ–‡ç¿»è¯‘
    """
    if not DEEPSEEK_API_KEY:
        print("è­¦å‘Šï¼šæœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨ç®€å•ç¿»è¯‘")
        return None

    try:
        from openai import OpenAI

        client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL
        )

        prompt = f"""è¯·å°†ä»¥ä¸‹æ–°é—»æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼Œè¦æ±‚ç®€æ´ã€å‡†ç¡®ã€ä¸“ä¸šã€‚æ ‡é¢˜ï¼š{title}

æ³¨æ„ï¼š
1. åªè¾“å‡ºç¿»è¯‘åçš„ä¸­æ–‡æ ‡é¢˜ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹
2. ä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§ï¼ˆå¦‚ AIã€Claudeã€ChatGPT ç­‰ï¼‰
3. å…¬å¸åç§°å¯ä»¥ä¿ç•™è‹±æ–‡æˆ–ä½¿ç”¨ä¸­æ–‡é€šè¯‘
4. ç¿»è¯‘è¦ç®€æ´ï¼Œç¬¦åˆä¸­æ–‡æ–°é—»æ ‡é¢˜çš„ä¹ æƒ¯"""

        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=200
        )

        translated = response.choices[0].message.content.strip()
        return translated

    except ImportError:
        print("è­¦å‘Šï¼šæœªå®‰è£… openai åº“ï¼Œä½¿ç”¨ç®€å•ç¿»è¯‘")
        return None
    except Exception as e:
        print(f"DeepSeek API è°ƒç”¨å¤±è´¥: {e}")
        return None


def translate_title(title: str) -> str:
    """
    ç¿»è¯‘è‹±æ–‡æ ‡é¢˜ä¸ºä¸­æ–‡

    Args:
        title: è‹±æ–‡æ ‡é¢˜

    Returns:
        ä¸­æ–‡æ ‡é¢˜ï¼ˆå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å›åŸæ–‡ï¼‰
    """
    # å…ˆè§£ç  HTML å®ä½“
    decoded_title = html.unescape(title)

    # è§„èŒƒåŒ–å¼•å·ï¼šå°† curly quotes è½¬æ¢ä¸º straight quotes
    normalized_title = normalize_quotes(decoded_title)

    # å…ˆå°è¯•å®Œå…¨åŒ¹é…ï¼ˆä½¿ç”¨è§„èŒƒåŒ–çš„æ ‡é¢˜ï¼‰
    if normalized_title in TITLE_TRANSLATIONS:
        return TITLE_TRANSLATIONS[normalized_title]

    # ç§»é™¤å¤šä½™ç©ºç™½
    clean_title = ' '.join(normalized_title.split())

    if clean_title in TITLE_TRANSLATIONS:
        return TITLE_TRANSLATIONS[clean_title]

    # ç®€å•ç¿»è¯‘ï¼šæå–å…³é”®è¯
    keywords = {
        "Google": "è°·æ­Œ", "OpenAI": "OpenAI", "Anthropic": "Anthropic",
        "Claude": "Claude", "ChatGPT": "ChatGPT", "Gemini": "Gemini",
        "AI": "AI", "raises": "èèµ„", "raise": "èèµ„", "investment": "æŠ•èµ„",
        "launch": "å‘å¸ƒ", "released": "å‘å¸ƒ", "revenue": "æ”¶å…¥", "users": "ç”¨æˆ·",
        "billion": "åäº¿", "million": "ç™¾ä¸‡", "tops": "çªç ´", "surpassed": "è¶…è¿‡",
        "monthly active": "æœˆæ´»è·ƒ", "app": "åº”ç”¨", "ads": "å¹¿å‘Š", "ad": "å¹¿å‘Š",
        "extension": "æ‰©å±•", "security": "å®‰å…¨", "nightmare": "å™©æ¢¦",
        "coding": "ç¼–ç¨‹", "assistant": "åŠ©æ‰‹", "available": "å¯ç”¨", "testing": "æµ‹è¯•",
        "tools": "å·¥å…·", "production": "åˆ¶ä½œ", "infrastructure": "åŸºç¡€è®¾æ–½",
        "valuation": "ä¼°å€¼", "plans": "è®¡åˆ’", "spending": "æŠ•å…¥", "race": "ç«èµ›",
    }

    result = normalized_title
    for en, zh in keywords.items():
        result = re.sub(r'\b' + en + r'\b', zh, result, flags=re.IGNORECASE)

    return result if result != normalized_title else normalized_title


def extract_key_news(items: List[Dict], source_filter: str = None, limit: int = 3) -> str:
    """
    æå–å…³é”®æ–°é—»æ ‡é¢˜ï¼ˆç”¨äºæ‘˜è¦ï¼‰

    Args:
        items: æ–°é—»åˆ—è¡¨
        source_filter: æ¥æºè¿‡æ»¤å™¨
        limit: æœ€å¤šæå–å¤šå°‘æ¡

    Returns:
        å…³é”®æ–°é—»æ‘˜è¦å­—ç¬¦ä¸²ï¼ˆç”¨åˆ†å·è¿æ¥ï¼‰
    """
    # è¿‡æ»¤æ¥æº
    if source_filter:
        filtered = [item for item in items if source_filter.lower() in item.get("æ¥æº", "").lower()]
    else:
        filtered = items

    # æå–å‰å‡ æ¡æ–°é—»ä½œä¸ºå…³é”®æ–°é—»
    key_news = []
    for item in filtered[:limit]:
        title = item.get("æ ‡é¢˜", "")

        # ä¼˜å…ˆä½¿ç”¨ DeepSeek API ç¿»è¯‘
        translated = translate_with_deepseek(title)
        if not translated:
            # å¦‚æœ API ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°ç®€å•ç¿»è¯‘
            translated = translate_title(title)

        key_news.append(translated)

    return "ï¼›".join(key_news)


def generate_summary_html(items: List[Dict]) -> str:
    """
    ç”Ÿæˆè¯¦ç»†çš„ HTML æ‘˜è¦

    Args:
        items: æ–°é—»åˆ—è¡¨

    Returns:
        HTML æ ¼å¼çš„æ‘˜è¦
    """
    # ç»Ÿè®¡å„å¹³å°æ•°é‡
    stats = {
        "verge": 0,
        "techcrunch": 0,
        "nyt": 0,
        "youtube": 0,
        "total": len(items)
    }

    # æŒ‰æ¥æºåˆ†ç»„
    by_source = {
        "The Verge AI": [],
        "TechCrunch AI": [],
        "NYT AI": [],
        "YouTube": []
    }

    for item in items:
        source = item.get("æ¥æº", "")
        if "verge" in source.lower():
            by_source["The Verge AI"].append(item)
            stats["verge"] += 1
        elif "techcrunch" in source.lower():
            by_source["TechCrunch AI"].append(item)
            stats["techcrunch"] += 1
        elif "nyt" in source.lower():
            by_source["NYT AI"].append(item)
            stats["nyt"] += 1
        elif "youtube" in source.lower():
            by_source["YouTube"].append(item)
            stats["youtube"] += 1

    # è·å–ä»Šå¤©çš„æ—¥æœŸ
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

    # ç”Ÿæˆ HTML
    html_parts = []

    # æ ‡é¢˜ç”± update_index_html ä¸­çš„æ­£åˆ™ä¿ç•™ï¼Œè¿™é‡Œä¸ç”Ÿæˆ
    # html_parts.append(f'            <h2>ğŸ“Š ä»Šæ—¥çƒ­ç‚¹æ‘˜è¦</h2>')
    html_parts.append(f'            <p class="highlight">{today}æ›´æ–° Â· å…± {stats["total"]} æ¡å†…å®¹</p>')

    # The Verge AI
    if by_source["The Verge AI"]:
        verge_summary = extract_key_news(items, "verge", limit=3)
        html_parts.append(f'            <h3>ğŸ“° The Verge AIï¼ˆ{stats["verge"]}æ¡ï¼‰</h3>')
        html_parts.append(f'            <p>{verge_summary}</p>')

    # TechCrunch AI
    if by_source["TechCrunch AI"]:
        tc_summary = extract_key_news(items, "techcrunch", limit=3)
        html_parts.append(f'            <h3>ğŸ’° TechCrunch AIï¼ˆ{stats["techcrunch"]}æ¡ï¼‰</h3>')
        html_parts.append(f'            <p>{tc_summary}</p>')

    # NYT AI
    if by_source["NYT AI"]:
        nyt_summary = extract_key_news(items, "nyt", limit=3)
        html_parts.append(f'            <h3>ğŸ›ï¸ NYT AIï¼ˆ{stats["nyt"]}æ¡ï¼‰</h3>')
        html_parts.append(f'            <p>{nyt_summary}</p>')

    # YouTube
    if by_source["YouTube"]:
        youtube_items = by_source["YouTube"]
        channels = list(set([item.get("æ¥æº", "").replace("YouTube - ", "") for item in youtube_items]))
        channel_count = len(channels)
        video_count = stats["youtube"]

        # æå–ä¸»é¢˜
        all_titles = " ".join([item.get("æ ‡é¢˜", "") for item in youtube_items])
        topics = []

        topic_keywords = {
            "å•†ä¸š|åˆ›ä¸š|business|entrepreneur": "AI å•†ä¸šä¸åˆ›ä¸š",
            "æ¼”ç¤º|demo|project|showcase": "AI é¡¹ç›®æ¼”ç¤º",
            "ç†è®º|ç ”ç©¶|paper|research": "AI ç†è®ºä¸ç ”ç©¶",
            "è¶‹åŠ¿|å±•æœ›|future|trend": "AI è¶‹åŠ¿ä¸å±•æœ›",
            "æ•™ç¨‹|tutorial|how to|guide": "AI æ•™ç¨‹å­¦ä¹ ",
            "ç¼–ç¨‹|coding|programming": "AI ç¼–ç¨‹å¼€å‘",
        }

        for pattern, topic in topic_keywords.items():
            if re.search(pattern, all_titles, re.IGNORECASE):
                topics.append(topic)

        # æ£€æµ‹å…¬å¸
        companies = []
        company_keywords = {
            "google|gemini": "Google/Gemini",
            "openai|chatgpt": "OpenAI",
            "anthropic|claude": "Anthropic/Claude",
            "deepseek": "DeepSeek",
        }

        for pattern, company in company_keywords.items():
            if re.search(pattern, all_titles, re.IGNORECASE):
                companies.append(company)

        topic_str = "ã€".join(topics[:4]) if topics else "AI ç›¸å…³å†…å®¹"
        company_str = "ã€".join(companies[:2]) if companies else "ä¸»æµ AI"

        html_parts.append(f'            <h3>ğŸ¥ YouTubeï¼ˆ{stats["youtube"]}æ¡ï¼‰</h3>')
        html_parts.append(f'            <p>{channel_count}ä½åšä¸»å‘å¸ƒ{video_count}ä¸ªè§†é¢‘ï¼Œæ¶µç›–{topic_str}ç­‰å†…å®¹ï¼Œæ¶‰åŠ{company_str}ç­‰å…¬å¸äº§å“</p>')

    return "\n".join(html_parts)


def update_index_html(summary_html: str, index_path: str = None):
    """
    æ›´æ–° index.html æˆ– today.html ä¸­çš„æ‘˜è¦éƒ¨åˆ†

    Args:
        summary_html: ç”Ÿæˆçš„æ‘˜è¦ HTML
        index_path: HTML æ–‡ä»¶è·¯å¾„
    """
    index_path = Path(index_path)

    # è¯»å– HTML æ–‡ä»¶
    with open(index_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æŸ¥æ‰¾å¹¶æ›¿æ¢æ‘˜è¦éƒ¨åˆ†ï¼ˆæ›´ç²¾ç¡®çš„æ¨¡å¼ï¼Œå®Œå…¨æ›¿æ¢æ‘˜è¦å†…å®¹ï¼‰
    # åŒ¹é…ä» <div class="summary-section"> ä¹‹åï¼Œåˆ°ç¬¬ä¸€ä¸ª </div> ç»“æŸï¼ˆæ‘˜è¦éƒ¨åˆ†çš„ç»“æŸï¼‰
    pattern = r'(<div class="summary-section">\s*<h2>ğŸ“Š ä»Šæ—¥çƒ­ç‚¹æ‘˜è¦</h2>\s*).*?(</div>\s*(?=<div class="filter-tabs">|<div id="newsContainer"|<script>|$))'

    def replace_summary(match):
        # åªä¿ç•™å¼€å¤´çš„ div æ ‡ç­¾å’Œ h2 æ ‡é¢˜ï¼Œç„¶åæ’å…¥æ–°çš„æ‘˜è¦å†…å®¹
        return match.group(1) + summary_html + "\n        " + match.group(2)

    new_content = re.sub(pattern, replace_summary, content, count=1, flags=re.DOTALL)

    # å†™å›æ–‡ä»¶
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(new_content)

    print(f"âœ… å·²æ›´æ–° {index_path}")


def update_app_js(items: List[Dict], app_js_path: str = None):
    """
    æ›´æ–° app.js ä¸­çš„æ–°é—»æ•°æ®

    Args:
        items: æ–°é—»åˆ—è¡¨
        app_js_path: app.js æ–‡ä»¶è·¯å¾„
    """
    if app_js_path is None:
        project_root = Path(__file__).parent.parent
        app_js_path = project_root / "app.js"

    app_js_path = Path(app_js_path)

    # ç”Ÿæˆ JavaScript æ•°æ® - éœ€è¦è½¬ä¹‰æ¢è¡Œç¬¦
    def escape_js_string(s):
        """è½¬ä¹‰ JavaScript å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
        if not isinstance(s, str):
            return s
        # è½¬ä¹‰æ¢è¡Œç¬¦å’Œå…¶ä»–ç‰¹æ®Šå­—ç¬¦
        s = s.replace('\\', '\\\\')  # åæ–œæ å¿…é¡»å…ˆè½¬ä¹‰
        s = s.replace('\n', '\\n')   # æ¢è¡Œ
        s = s.replace('\r', '\\r')   # å›è½¦
        s = s.replace('\t', '\\t')   # åˆ¶è¡¨ç¬¦
        s = s.replace('"', '\\"')    # åŒå¼•å·
        return s

    # æ‰‹åŠ¨æ„å»º JavaScript æ•°ç»„
    js_lines = ['const newsData = [']
    for item in items:
        js_lines.append('    {')
        for key, value in item.items():
            if isinstance(value, str):
                escaped_value = escape_js_string(value)
                js_lines.append(f'"{key}": "{escaped_value}",')
            else:
                js_lines.append(f'"{key}": {value},')
        # ç§»é™¤æœ€åä¸€è¡Œçš„é€—å·
        js_lines[-1] = js_lines[-1].rstrip(',')
        js_lines.append('    },')
    js_lines.append('];')

    js_data = '\n'.join(js_lines)

    # è¯»å– app.js
    with open(app_js_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æ›¿æ¢ newsData æ•°ç»„
    pattern = r'const newsData = \[.*?\];'
    new_data = js_data

    new_content = re.sub(pattern, new_data, content, flags=re.DOTALL)

    # å†™å›æ–‡ä»¶
    with open(app_js_path, 'w', encoding='utf-8') as f:
        f.write(new_content)

    print(f"âœ… å·²æ›´æ–° {app_js_path.name}")


def main():
    """ä¸»å‡½æ•°"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent
    news_json_path = project_root / "output" / "news.json"

    # è¯»å–æ–°é—»æ•°æ®
    if not news_json_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {news_json_path}")
        print("è¯·å…ˆè¿è¡Œ python3 scripts/run_aggregator.py ç”Ÿæˆæ•°æ®")
        return

    with open(news_json_path, 'r', encoding='utf-8') as f:
        items = json.load(f)

    print(f"ğŸ“Š è¯»å–åˆ° {len(items)} æ¡æ–°é—»")

    # ç”Ÿæˆæ‘˜è¦ HTML
    summary_html = generate_summary_html(items)

    print("\nğŸ“ ç”Ÿæˆçš„æ‘˜è¦é¢„è§ˆï¼š")
    print(summary_html)
    print("\n")

    # ç¬¬ä¸€æ­¥ï¼šæ›´æ–°æœ¬åœ°æ–‡ä»¶
    print("ğŸ”„ ç¬¬ä¸€æ­¥ï¼šæ›´æ–°æœ¬åœ°æ–‡ä»¶...")

    # æ›´æ–° output/today.htmlï¼ˆæœ¬åœ°æœåŠ¡å™¨ï¼‰
    today_path = project_root / "output" / "today.html"
    update_index_html(summary_html, today_path)

    # æ›´æ–° app.jsï¼ˆæ ¹ç›®å½•ï¼Œç”¨äº GitHub Pagesï¼‰
    update_app_js(items)

    print("\nâœ… æœ¬åœ°æ–‡ä»¶æ›´æ–°å®Œæˆï¼")
    print(f"   - {today_path.name} (æœ¬åœ°æœåŠ¡å™¨)")
    print(f"   - app.js (GitHub Pages æ•°æ®)")

    # ç¬¬äºŒæ­¥ï¼šæ›´æ–°æ ¹ç›®å½• index.html
    print("\nğŸ”„ ç¬¬äºŒæ­¥ï¼šæ›´æ–° GitHub Pages æ–‡ä»¶...")

    index_path = project_root / "index.html"
    update_index_html(summary_html, index_path)

    print(f"   - {index_path.name} (GitHub Pages)")

    print("\nâœ… å…¨éƒ¨æ›´æ–°å®Œæˆï¼")
    print("\nğŸ“Œ ä¸‹ä¸€æ­¥ï¼š")
    print("   1. æœ¬åœ°æµ‹è¯•ï¼šè®¿é—® http://127.0.0.1:5000/")
    print("   2. ç¡®è®¤æ— è¯¯åï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¨é€åˆ° GitHubï¼š")
    print("      git add -A")
    print("      git commit -m 'æ›´æ–° AI çƒ­ç‚¹æ‘˜è¦'")
    print("      git push")


if __name__ == "__main__":
    main()
