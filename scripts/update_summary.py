"""
AI News Aggregator - æ‘˜è¦ç”Ÿæˆå™¨
è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„ AI çƒ­ç‚¹æ‘˜è¦å¹¶æ›´æ–°åˆ° index.html
"""

import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any


def extract_key_news(items: List[Dict], source_filter: str = None, limit: int = 3) -> str:
    """
    æå–å…³é”®æ–°é—»æ ‡é¢˜ï¼ˆç”¨äºæ‘˜è¦ï¼‰

    Args:
        items: æ–°é—»åˆ—è¡¨
        source_filter: æ¥æºè¿‡æ»¤å™¨
        limit: æœ€å¤šæå–å¤šå°‘æ¡

    Returns:
        å…³é”®æ–°é—»æ‘˜è¦å­—ç¬¦ä¸²ï¼ˆç”¨åˆ†å·è¿æ¥ï¼‰
    """
    # è¿‡æ»¤æ¥æº
    if source_filter:
        filtered = [item for item in items if source_filter.lower() in item.get("æ¥æº", "").lower()]
    else:
        filtered = items

    # æå–å‰å‡ æ¡æ–°é—»ä½œä¸ºå…³é”®æ–°é—»
    key_news = []
    for item in filtered[:limit]:
        title = item.get("æ ‡é¢˜", "")
        # ç§»é™¤ HTML å®ä½“
        title = re.sub(r'&#\d+;', '', title)
        # ç§»é™¤å¤šä½™ç©ºç™½
        title = ' '.join(title.split())
        key_news.append(title)

    return "ï¼›".join(key_news)


def generate_summary_html(items: List[Dict]) -> str:
    """
    ç”Ÿæˆè¯¦ç»†çš„ HTML æ‘˜è¦

    Args:
        items: æ–°é—»åˆ—è¡¨

    Returns:
        HTML æ ¼å¼çš„æ‘˜è¦
    """
    # ç»Ÿè®¡å„å¹³å°æ•°é‡
    stats = {
        "verge": 0,
        "techcrunch": 0,
        "nyt": 0,
        "youtube": 0,
        "total": len(items)
    }

    # æŒ‰æ¥æºåˆ†ç»„
    by_source = {
        "The Verge AI": [],
        "TechCrunch AI": [],
        "NYT AI": [],
        "YouTube": []
    }

    for item in items:
        source = item.get("æ¥æº", "")
        if "verge" in source.lower():
            by_source["The Verge AI"].append(item)
            stats["verge"] += 1
        elif "techcrunch" in source.lower():
            by_source["TechCrunch AI"].append(item)
            stats["techcrunch"] += 1
        elif "nyt" in source.lower():
            by_source["NYT AI"].append(item)
            stats["nyt"] += 1
        elif "youtube" in source.lower():
            by_source["YouTube"].append(item)
            stats["youtube"] += 1

    # è·å–ä»Šå¤©çš„æ—¥æœŸ
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

    # ç”Ÿæˆ HTML
    html_parts = []

    # æ ‡é¢˜
    html_parts.append(f'            <h2>ğŸ“Š ä»Šæ—¥çƒ­ç‚¹æ‘˜è¦</h2>')
    html_parts.append(f'            <p class="highlight">{today}æ›´æ–° Â· å…± {stats["total"]} æ¡å†…å®¹</p>')

    # The Verge AI
    if by_source["The Verge AI"]:
        verge_summary = extract_key_news(items, "verge", limit=3)
        html_parts.append(f'            <h3>ğŸ“° The Verge AIï¼ˆ{stats["verge"]}æ¡ï¼‰</h3>')
        html_parts.append(f'            <p>{verge_summary}</p>')

    # TechCrunch AI
    if by_source["TechCrunch AI"]:
        tc_summary = extract_key_news(items, "techcrunch", limit=3)
        html_parts.append(f'            <h3>ğŸ’° TechCrunch AIï¼ˆ{stats["techcrunch"]}æ¡ï¼‰</h3>')
        html_parts.append(f'            <p>{tc_summary}</p>')

    # NYT AI
    if by_source["NYT AI"]:
        nyt_summary = extract_key_news(items, "nyt", limit=3)
        html_parts.append(f'            <h3>ğŸ›ï¸ NYT AIï¼ˆ{stats["nyt"]}æ¡ï¼‰</h3>')
        html_parts.append(f'            <p>{nyt_summary}</p>')

    # YouTube
    if by_source["YouTube"]:
        youtube_items = by_source["YouTube"]
        channels = list(set([item.get("æ¥æº", "").replace("YouTube - ", "") for item in youtube_items]))
        channel_count = len(channels)
        video_count = stats["youtube"]

        # æå–ä¸»é¢˜
        all_titles = " ".join([item.get("æ ‡é¢˜", "") for item in youtube_items])
        topics = []

        topic_keywords = {
            "å•†ä¸š|åˆ›ä¸š|business|entrepreneur": "AI å•†ä¸šä¸åˆ›ä¸š",
            "æ¼”ç¤º|demo|project|showcase": "AI é¡¹ç›®æ¼”ç¤º",
            "ç†è®º|ç ”ç©¶|paper|research": "AI ç†è®ºä¸ç ”ç©¶",
            "è¶‹åŠ¿|å±•æœ›|future|trend": "AI è¶‹åŠ¿ä¸å±•æœ›",
            "æ•™ç¨‹|tutorial|how to|guide": "AI æ•™ç¨‹å­¦ä¹ ",
            "ç¼–ç¨‹|coding|programming": "AI ç¼–ç¨‹å¼€å‘",
        }

        for pattern, topic in topic_keywords.items():
            if re.search(pattern, all_titles, re.IGNORECASE):
                topics.append(topic)

        # æ£€æµ‹å…¬å¸
        companies = []
        company_keywords = {
            "google|gemini": "Google/Gemini",
            "openai|chatgpt": "OpenAI",
            "anthropic|claude": "Anthropic/Claude",
            "deepseek": "DeepSeek",
        }

        for pattern, company in company_keywords.items():
            if re.search(pattern, all_titles, re.IGNORECASE):
                companies.append(company)

        topic_str = "ã€".join(topics[:4]) if topics else "AI ç›¸å…³å†…å®¹"
        company_str = "ã€".join(companies[:2]) if companies else "ä¸»æµ AI"

        html_parts.append(f'            <h3>ğŸ¥ YouTubeï¼ˆ{stats["youtube"]}æ¡ï¼‰</h3>')
        html_parts.append(f'            <p>{channel_count}ä½åšä¸»å‘å¸ƒ{video_count}ä¸ªè§†é¢‘ï¼Œæ¶µç›–{topic_str}ç­‰å†…å®¹ï¼Œæ¶‰åŠ{company_str}ç­‰å…¬å¸äº§å“</p>')

    return "\n".join(html_parts)


def update_index_html(summary_html: str, index_path: str = None):
    """
    æ›´æ–° index.html ä¸­çš„æ‘˜è¦éƒ¨åˆ†

    Args:
        summary_html: ç”Ÿæˆçš„æ‘˜è¦ HTML
        index_path: index.html æ–‡ä»¶è·¯å¾„
    """
    if index_path is None:
        # é»˜è®¤è·¯å¾„
        project_root = Path(__file__).parent.parent
        index_path = project_root / "index.html"

    index_path = Path(index_path)

    # è¯»å– index.html
    with open(index_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æŸ¥æ‰¾å¹¶æ›¿æ¢æ‘˜è¦éƒ¨åˆ†
    pattern = r'(<div class="summary-section">.*?<h2>ğŸ“Š ä»Šæ—¥çƒ­ç‚¹æ‘˜è¦</h2>).*?(</div>\s*</div>)'

    def replace_summary(match):
        return match.group(1) + "\n" + summary_html + "\n        " + match.group(2)

    new_content = re.sub(pattern, replace_summary, content, flags=re.DOTALL)

    # å†™å›æ–‡ä»¶
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(new_content)

    print(f"âœ… å·²æ›´æ–° {index_path}")


def main():
    """ä¸»å‡½æ•°"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent
    news_json_path = project_root / "output" / "news.json"

    # è¯»å–æ–°é—»æ•°æ®
    if not news_json_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {news_json_path}")
        print("è¯·å…ˆè¿è¡Œ python3 scripts/run_aggregator.py ç”Ÿæˆæ•°æ®")
        return

    with open(news_json_path, 'r', encoding='utf-8') as f:
        items = json.load(f)

    print(f"ğŸ“Š è¯»å–åˆ° {len(items)} æ¡æ–°é—»")

    # ç”Ÿæˆæ‘˜è¦ HTML
    summary_html = generate_summary_html(items)

    print("\nğŸ“ ç”Ÿæˆçš„æ‘˜è¦é¢„è§ˆï¼š")
    print(summary_html)
    print("\n")

    # æ›´æ–°æ ¹ç›®å½•çš„ index.htmlï¼ˆç”¨äº GitHub Pagesï¼‰
    index_path = project_root / "index.html"
    update_index_html(summary_html, index_path)

    # æ›´æ–° output/today.htmlï¼ˆç”¨äºæœ¬åœ°æœåŠ¡å™¨ï¼‰
    today_path = project_root / "output" / "today.html"
    update_index_html(summary_html, today_path)

    print("âœ… æ‘˜è¦æ›´æ–°å®Œæˆï¼")
    print("ğŸ“Œ å·²åŒæ­¥æ›´æ–°:")
    print(f"   - {index_path} (GitHub Pages)")
    print(f"   - {today_path} (æœ¬åœ°æœåŠ¡å™¨)")


if __name__ == "__main__":
    main()
